\documentclass[12pt, a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{float}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{setspace}
% \usepackage{float}       % Pour l'option [H] (placement exact)
% \usepackage{subcaption}  % Pour mettre les images côte à côte (subfigure)
% \usepackage{graphicx}   % Pour inclure les images


\geometry{a4paper, margin=1in}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\nouppercase{\leftmark}}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}
\onehalfspacing

\theoremstyle{definition}
\newtheorem{definition}{Définition}[section]
\newtheorem{theorem}{Théorème}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemme}{Lemme}[section]

\lstset{
    language=R,
    basicstyle=\ttfamily\footnotesize,
    numbers=left,
    numberstyle=\scriptsize\color{gray},
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    breakatwhitespace=true,
    columns=fullflexible,
    frame=single,
    captionpos=b,
    keywordstyle=\color{blue},
    commentstyle=\color{green!60!black},
    stringstyle=\color{red!70!black}
}

\title{Projet Académique Avancé en Économétrie et Statistique : \\ \vspace{0.5cm} La Modélisation du Prix des Biens Immobiliers par Régression Linéaire Multiple et Méthodes Régularisées}

\author{
Rédigé par : BEN AKKA OUAYAD Mohammed \\ 
BEN FARES Mohamed \\[0.3cm]
Encadré par : Prof. Abdelkamel ALJ \\[0.3cm]
}

\date{Janvier 2026}

\begin{document}

\maketitle
\thispagestyle{empty}
\newpage

\tableofcontents
\newpage

\chapter{Introduction Générale}

\section{Contexte et Motivation de l'Étude}
L'évaluation immobilière est un pilier de l'économie moderne. La détermination du prix d'un bien est un problème multidimensionnel, influencé par des facteurs hédoniques (caractéristiques du bien), macroéconomiques et géographiques. Ce projet s'inscrit dans une démarche d'économétrie appliquée visant à décortiquer ces influences. L'objectif n'est pas seulement de prédire, mais d'interpréter la contribution marginale de chaque facteur, un impératif pour les décideurs, les investisseurs et les chercheurs [1].

\section{Importance du Phénomène Étudié}
Le marché immobilier du comté de King, incluant Seattle, est emblématique des dynamiques de croissance rapide et de forte disparité des prix. L'étude de ce marché offre un terrain fertile pour l'application de modèles statistiques avancés. Une modélisation rigoureuse permet de décomposer le prix en ses composantes structurelles et de localisation, offrant une transparence essentielle dans un marché souvent opaque.

\section{Justification du Recours à l'Analyse Statistique}
L'approche statistique, et en particulier la Régression Linéaire Multiple (RLM), est indispensable pour isoler l'effet de chaque variable explicative (\textit{ceteris paribus}). Elle permet de passer d'une simple corrélation à une inférence causale (sous réserve de la validité du modèle), quantifiant l'élasticité du prix par rapport à des variables clés comme la surface habitable ou la qualité de construction.

\section{Problématique et Question Centrale}
\textbf{Problématique :} Comment peut-on construire un modèle économétrique robuste, interprétable et validé par des diagnostics rigoureux, capable de quantifier l'impact des caractéristiques hédoniques et géographiques sur le prix de vente des maisons, tout en gérant les défis statistiques inhérents aux données réelles (multicolinéarité, hétéroscédasticité) ?

\textbf{Question Centrale du Projet :} Quel est le modèle de régression (RLM ou régularisé) qui offre le meilleur compromis entre pouvoir prédictif, respect des hypothèses classiques et interprétabilité économique pour les données immobilières du comté de King ?

\section{Objectif Général et Objectifs Spécifiques}
\textbf{Objectif Général :} Développer, valider et comparer des modèles de régression pour la prédiction et l'interprétation du prix des maisons, en utilisant des techniques statistiques avancées.

\textbf{Objectifs Spécifiques :}
\begin{enumerate}
    \item Mener une Analyse Exploratoire des Données (AED) exhaustive, incluant la détection et le traitement des valeurs aberrantes et influentes.
    \item Établir les fondements théoriques et mathématiques de la RLM (formulation matricielle, propriétés des estimateurs MCO).
    \item Construire un modèle de RLM et effectuer une série complète de diagnostics pour valider les hypothèses (normalité, homoscédasticité, multicolinéarité).
    \item Explorer et comparer les performances du modèle RLM avec des modèles régularisés (Ridge et Lasso).
    \item Fournir une interprétation économétrique détaillée des résultats et une discussion critique des limites.
\end{enumerate}

\section{Organisation du Rapport}
Ce rapport est structuré en sept chapitres. Le Chapitre 2 présente les données. Le Chapitre 3 est dédié aux fondements théoriques de la RLM. Le Chapitre 4 expose la méthodologie d'analyse avancée. Le Chapitre 5 présente les résultats de l'analyse sous R. Le Chapitre 6 propose une discussion économétrique. Enfin, le Chapitre 7 conclut et ouvre des perspectives. Des annexes détaillées complètent l'analyse.

\newpage

\chapter{Présentation et Analyse Exploratoire des Données (AED)}

\section{Description du Jeu de Données}
\subsection{Source et Caractéristiques Générales}
Le jeu de données sélectionné est \textbf{House Sales in King County, USA} [2], disponible sur Kaggle. Il contient 21 613 observations de ventes de maisons entre mai 2014 et mai 2015. La richesse de ce jeu de données réside dans ses 21 variables, couvrant les aspects structurels, de localisation et de qualité.

\subsection{Description Détaillée des Variables}
La variable dépendante est \texttt{price}. Les variables explicatives clés pour la modélisation sont présentées dans le Tableau \ref{tab:variables_detail}.

\begin{longtable}{|p{3cm}|p{2cm}|p{9cm}|}
    \caption{Description Détaillée des Variables Clés}
    \label{tab:variables_detail} \\
    \hline
    \textbf{Variable} & \textbf{Type} & \textbf{Description} \\
    \hline
    \endfirsthead
    \caption{Description Détaillée des Variables Clés (suite)} \\
    \hline
    \textbf{Variable} & \textbf{Type} & \textbf{Description} \\
    \hline
    \endhead
    \hline
    \endfoot
    \hline
    \endlastfoot
    \texttt{price} & Quantitative & Prix de vente en dollars (Variable Cible). \\
    \texttt{sqft\_living} & Quantitative & Surface habitable intérieure en pieds carrés. \\
    \texttt{bedrooms} & Discrète & Nombre de chambres. \\
    \texttt{bathrooms} & Discrète & Nombre de salles de bain (avec incréments de 0.25). \\
    \texttt{floors} & Discrète & Nombre d'étages. \\
    \texttt{waterfront} & Binaire & 1 si la propriété a une vue sur l'eau, 0 sinon. \\
    \texttt{view} & Ordinale & Indice de qualité de la vue (0 à 4). \\
    \texttt{condition} & Ordinale & État général de la maison (1 à 5). \\
    \texttt{grade} & Ordinale & Indice de qualité de construction et de conception (1 à 13). \\
    \texttt{sqft\_above} & Quantitative & Surface au-dessus du sol. \\
    \texttt{sqft\_basement} & Quantitative & Surface du sous-sol. \\
    \texttt{yr\_built} & Discrète & Année de construction. \\
    \texttt{lat}, \texttt{long} & Quantitative & Coordonnées géographiques (Latitude et Longitude). \\
\end{longtable}

\section{Analyse Univariée et Transformation des Données}
\subsection{Distribution de la Variable Cible}
L'histogramme de \texttt{price} (Figure \ref{fig:distribution_prix}) révèle une forte asymétrie positive (skewness). La majorité des maisons se vendent à des prix relativement bas, avec une longue queue de valeurs extrêmes. Cette distribution non normale est typique des données de prix et viole l'hypothèse de normalité des erreurs si elle n'est pas traitée.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{regression_output/distribution_prix_avant_apres.png}
    \caption{Distribution des prix de vente : Avant et Après Transformation Logarithmique}
    \label{fig:distribution_prix}
\end{figure}

\subsection{Transformation Logarithmique}
Pour stabiliser la variance et rendre la distribution plus symétrique, la variable cible est transformée en $\ln(\text{price})$. Le modèle estimé sera donc un modèle log-linéaire, où les coefficients s'interprètent en termes d'élasticité ou de semi-élasticité.

\section{Analyse Bivariée et Multivariée}
\subsection{Matrice de Corrélation}
La matrice de corrélation des variables quantitatives (Figure \ref{fig:corr_matrix}) est essentielle pour identifier les relations linéaires.
\begin{itemize}
    \item Une forte corrélation positive est observée entre $\ln(\text{price})$ et \texttt{sqft\_living} ($\rho \approx 0.70$) et \texttt{grade} ($\rho \approx 0.67$).
    \item Des corrélations élevées entre variables explicatives sont notées, notamment entre \texttt{sqft\_living} et \texttt{sqft\_above} ($\rho \approx 0.88$), et entre \texttt{sqft\_living} et \texttt{grade} ($\rho \approx 0.76$). Ces signaux indiquent un risque de \textbf{multicolinéarité} qui devra être géré.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{regression_output/matrice_correlation.png}
    \caption{Matrice de Corrélation des Variables Clés (Simulée)}
    \label{fig:corr_matrix}
\end{figure}

\section{Détection et Traitement des Valeurs Influentes}
\subsection{Outliers et Points de Levier}
L'AED a permis d'identifier des observations potentiellement problématiques.
\begin{itemize}
    \item \textbf{Outliers :} Observations avec des prix ou des caractéristiques extrêmes (ex: maisons avec 33 chambres). Ces observations sont examinées et, si elles sont jugées non réalistes ou dues à des erreurs de saisie, elles sont retirées.
    \item \textbf{Points de Levier (Leverage Points) :} Observations éloignées de la moyenne des prédicteurs. Elles ont une forte influence potentielle sur l'estimation des coefficients.
\end{itemize}

\subsection{Distance de Cook}
La \textbf{Distance de Cook} est utilisée pour quantifier l'influence globale de chaque observation sur l'ensemble des coefficients du modèle. Les observations dont la distance de Cook dépasse un seuil critique (souvent $4/n$ ou $1$) sont considérées comme influentes et peuvent nécessiter une attention particulière ou une exclusion du modèle final.

\newpage

\chapter{Fondements Théoriques de la Régression Linéaire Multiple}

\section{Le Modèle de Régression Linéaire Multiple (RLM)}
\subsection{Formulation Matricielle}
Le modèle RLM est plus élégamment exprimé sous forme matricielle. Soit $n$ le nombre d'observations et $p$ le nombre de variables explicatives (incluant l'ordonnée à l'origine).
$$
\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}
$$
où :
\begin{itemize}
    \item $\mathbf{Y}$ est le vecteur $(n \times 1)$ des observations de la variable dépendante.
    \item $\mathbf{X}$ est la matrice $(n \times p)$ des prédicteurs, appelée matrice de design.
    \item $\boldsymbol{\beta}$ est le vecteur $(p \times 1)$ des coefficients de régression inconnus.
    \item $\boldsymbol{\varepsilon}$ est le vecteur $(n \times 1)$ des termes d'erreur.
\end{itemize}

\subsection{Hypothèses du Modèle Linéaire Classique (MLC)}
Les inférences statistiques reposent sur les hypothèses suivantes, souvent désignées par les hypothèses de Gauss-Markov :
\begin{enumerate}
    \item \textbf{Linéarité dans les Paramètres :} $\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}$.
    \item \textbf{Échantillon Aléatoire :} Les données $(\mathbf{x}_i, Y_i)$ sont un échantillon aléatoire de la population.
    \item \textbf{Absence de Multicolinéarité Parfaite :} $\text{rang}(\mathbf{X}) = p$. La matrice $\mathbf{X}$ est de plein rang colonne.
    \item \textbf{Espérance Nulle de l'Erreur :} $E(\boldsymbol{\varepsilon}|\mathbf{X}) = \mathbf{0}$. L'erreur est non corrélée avec les prédicteurs.
    \item \textbf{Homoscédasticité et Non-Autocorrélation :} $\text{Var}(\boldsymbol{\varepsilon}|\mathbf{X}) = \sigma^2 \mathbf{I}_n$. La variance des erreurs est constante ($\sigma^2$) et les erreurs sont non corrélées entre elles.
    \item \textbf{Normalité (pour l'inférence) :} $\boldsymbol{\varepsilon}|\mathbf{X} \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I}_n)$. Les erreurs sont normalement distribuées.
\end{enumerate}

\section{L'Estimateur des Moindres Carrés Ordinaires (MCO)}
\subsection{Dérivation de l'Estimateur MCO}
L'objectif de la méthode MCO est de trouver le vecteur de coefficients $\hat{\boldsymbol{\beta}}$ qui minimise la Somme des Carrés des Résidus (SCR) :
$$
\text{SCR}(\boldsymbol{\beta}) = \sum_{i=1}^n \hat{\varepsilon}_i^2 = \hat{\boldsymbol{\varepsilon}}^\top \hat{\boldsymbol{\varepsilon}} = (\mathbf{Y} - \mathbf{X}\boldsymbol{\beta})^\top (\mathbf{Y} - \mathbf{X}\boldsymbol{\beta})
$$
En annulant le gradient de la SCR par rapport à $\boldsymbol{\beta}$, on obtient les \textbf{équations normales} :
$$
\mathbf{X}^\top \mathbf{X} \hat{\boldsymbol{\beta}} = \mathbf{X}^\top \mathbf{Y}
$$
Sous l'hypothèse d'absence de multicolinéarité parfaite (Hypothèse 3), la matrice $\mathbf{X}^\top \mathbf{X}$ est inversible, et l'estimateur MCO est donné par :
$$
\hat{\boldsymbol{\beta}}_{\text{MCO}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y}
$$

\subsection{Le Théorème de Gauss-Markov}
Le Théorème de Gauss-Markov est la pierre angulaire de la RLM. Il établit les propriétés optimales de l'estimateur MCO.

\begin{theorem}[Gauss-Markov]
Sous les hypothèses 1 à 5 du Modèle Linéaire Classique (MLC), l'estimateur MCO $\hat{\boldsymbol{\beta}}_{\text{MCO}}$ est le Meilleur Estimateur Linéaire Sans Biais (MELSB) :
\begin{enumerate}
    \item \textbf{Linéaire :} $\hat{\boldsymbol{\beta}}_{\text{MCO}}$ est une fonction linéaire de $\mathbf{Y}$.
    \item \textbf{Sans Biais :} $E(\hat{\boldsymbol{\beta}}_{\text{MCO}}) = \boldsymbol{\beta}$.
    \item \textbf{Meilleur (Efficace) :} $\text{Var}(\hat{\boldsymbol{\beta}}_{\text{MCO}}) = \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}$ est la matrice de variance-covariance la plus petite parmi tous les estimateurs linéaires sans biais.
\end{enumerate}
\end{theorem}

\subsubsection{Démonstration de la Matrice de Variance-Covariance de l'Estimateur MCO}
Nous cherchons à déterminer la matrice de variance-covariance de $\hat{\boldsymbol{\beta}}_{\text{MCO}}$.
$$
\text{Var}(\hat{\boldsymbol{\beta}}_{\text{MCO}}|\mathbf{X}) = E\left[ (\hat{\boldsymbol{\beta}}_{\text{MCO}} - \boldsymbol{\beta})(\hat{\boldsymbol{\beta}}_{\text{MCO}} - \boldsymbol{\beta})^\top \middle| \mathbf{X} \right]
$$
Nous avons établi que $\hat{\boldsymbol{\beta}}_{\text{MCO}} - \boldsymbol{\beta} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \boldsymbol{\varepsilon}$.
$$
\text{Var}(\hat{\boldsymbol{\beta}}_{\text{MCO}}|\mathbf{X}) = E\left[ (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \boldsymbol{\varepsilon} \boldsymbol{\varepsilon}^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} \middle| \mathbf{X} \right]
$$
Sous l'hypothèse d'homoscédasticité et de non-autocorrélation (Hypothèse 5), $E(\boldsymbol{\varepsilon} \boldsymbol{\varepsilon}^\top|\mathbf{X}) = \text{Var}(\boldsymbol{\varepsilon}|\mathbf{X}) = \sigma^2 \mathbf{I}_n$.
$$
\text{Var}(\hat{\boldsymbol{\beta}}_{\text{MCO}}|\mathbf{X}) = \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} = \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}
$$

\subsubsection{Démonstration de la Propriété du Meilleur Estimateur Linéaire Sans Biais (MELSB)}
Soit $\tilde{\boldsymbol{\beta}} = \mathbf{A}\mathbf{Y}$ un autre estimateur linéaire sans biais, où $\mathbf{A}$ est une matrice $(p \times n)$. Pour que $\tilde{\boldsymbol{\beta}}$ soit sans biais, il faut que $\mathbf{A}\mathbf{X} = \mathbf{I}_p$.
La matrice de variance-covariance de $\tilde{\boldsymbol{\beta}}$ est $\text{Var}(\tilde{\boldsymbol{\beta}}) = \sigma^2 \mathbf{A}\mathbf{A}^\top$.
En définissant $\mathbf{D} = \mathbf{A} - (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top$, on montre que $\mathbf{D}\mathbf{X} = \mathbf{0}$.
On obtient alors :
$$
\text{Var}(\tilde{\boldsymbol{\beta}}) = \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1} + \sigma^2 \mathbf{D}\mathbf{D}^\top
$$
Puisque $\sigma^2 \mathbf{D}\mathbf{D}^\top$ est une matrice semi-définie positive, $\text{Var}(\tilde{\boldsymbol{\beta}}) - \text{Var}(\hat{\boldsymbol{\beta}}_{\text{MCO}}) \ge \mathbf{0}$. Ceci prouve que $\hat{\boldsymbol{\beta}}_{\text{MCO}}$ est le MELSB.

\section{Inférence Statistique et Tests d'Hypothèses}
\subsection{Distribution de l'Estimateur MCO}
Sous l'hypothèse de normalité (Hypothèse 6), l'estimateur MCO suit une distribution normale :
$$
\hat{\boldsymbol{\beta}}_{\text{MCO}} \sim \mathcal{N}(\boldsymbol{\beta}, \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1})
$$
Puisque $\sigma^2$ est inconnu, il est estimé par $\hat{\sigma}^2 = \text{SCR} / (n-p)$.

\subsection{Test de Significativité Individuelle (Test t)}
Pour tester l'hypothèse nulle $H_0: \beta_j = 0$, on utilise la statistique $t$ :
$$
t_j = \frac{\hat{\beta}_j - 0}{\text{se}(\hat{\beta}_j)} \sim t_{n-p}
$$

\subsection{Test de Significativité Globale (Test F)}
Le test F permet de tester l'hypothèse nulle que tous les coefficients de pente sont simultanément nuls : $H_0: \beta_1 = \beta_2 = \dots = \beta_{p-1} = 0$. La statistique F est donnée par :
$$
F = \frac{(\text{SCE} / (p-1))}{(\text{SCR} / (n-p))} \sim F_{p-1, n-p}
$$

\newpage

\chapter{Méthodologie Statistique Avancée et Régularisation}

\section{Diagnostic des Hypothèses du Modèle}
\subsection{Multicolinéarité}
\begin{definition}[Facteur d'Inflation de la Variance (VIF)]
Le VIF pour le $j$-ième prédicteur est défini comme :
$$
\text{VIF}_j = \frac{1}{1 - R_j^2}
$$
Une valeur de $\text{VIF}_j > 5$ ou $10$ est généralement considérée comme problématique.
\end{definition}

\subsection{Hétéroscédasticité}
L'hétéroscédasticité rend les erreurs standards MCO incorrectes.
\begin{itemize}
    \item \textbf{Test de Breusch-Pagan :} Teste $H_0: \text{Homoscédasticité}$.
    \item \textbf{Solution :} Utilisation des Erreurs Standards Robustes (ou de White) pour corriger les statistiques $t$ et $F$.
\end{itemize}

\subsection{Normalité des Résidus}
\begin{itemize}
    \item \textbf{QQ-Plot :} Représentation graphique des quantiles des résidus par rapport aux quantiles d'une distribution normale.
    \item \textbf{Test de Shapiro-Wilk :} Test formel de $H_0: \text{Les résidus sont normalement distribués}$.
\end{itemize}

\section{Sélection de Modèles et Critères d'Information}
\subsection{Critères d'Akaike (AIC) et Bayésien (BIC)}
$$
\text{AIC} = n \ln(\text{SCR}/n) + 2p \quad ; \quad \text{BIC} = n \ln(\text{SCR}/n) + p \ln(n)
$$

\section{Régression Régularisée (Shrinkage Methods)}
\subsection{Régression Ridge (Norme $L_2$)}
$$
\hat{\boldsymbol{\beta}}_{\text{Ridge}} = \underset{\boldsymbol{\beta}}{\arg\min} \left\{ \sum_{i=1}^n (Y_i - \mathbf{x}_i^\top \boldsymbol{\beta})^2 + \lambda \sum_{j=1}^p \beta_j^2 \right\}
$$

\subsection{Régression Lasso (Norme $L_1$)}
$$
\hat{\boldsymbol{\beta}}_{\text{Lasso}} = \underset{\boldsymbol{\beta}}{\arg\min} \left\{ \sum_{i=1}^n (Y_i - \mathbf{x}_i^\top \boldsymbol{\beta})^2 + \lambda \sum_{j=1}^p |\beta_j| \right\}
$$

\newpage

\chapter{Analyse et Résultats Sous R}

\section{Environnement de Travail et Préparation des Données}
\subsection{Packages et Script R}
L'analyse a été menée avec les packages R suivants : \texttt{tidyverse}, \texttt{corrplot}, \texttt{car}, \texttt{lmtest}, \texttt{ggplot2}, \texttt{glmnet}, \texttt{leaps}, \texttt{sandwich}, \texttt{stargazer}, \texttt{spdep} et \texttt{sf}. Le script R complet est fourni en Annexe B.

\section{Modèle de Régression Linéaire Multiple (RLM) Initial}
\subsection{Spécification du Modèle}
Le modèle initial (MCO) est spécifié comme suit :
\begin{verbatim}
log(price) ~ sqft_living + bedrooms + bathrooms + floors + 
            waterfront + view + condition + grade + yr_built + lat + long
\end{verbatim}

\subsection{Résultats de l'Estimation MCO}
Le Tableau \ref{tab:resultats_mco} présente les résultats simulés de l'estimation MCO.

 \begin{table}[H]
    \centering
    \caption{Résultats de l'Estimation MCO (Simulés)}
    \label{tab:resultats_mco}
    \begin{tabular}{lrrrr}
        \toprule
        \textbf{Variable} & \textbf{Coefficient} & \textbf{Erreur Std.} & \textbf{Statistique t} & \textbf{Valeur p} \\
        \midrule
        (Intercept) & -1.234e+02 & 1.234e+01 & -9.99 & < 2e-16 *** \\
        sqft\_living & 5.678e-04 & 1.234e-05 & 46.00 & < 2e-16 *** \\
        bedrooms & -1.234e-02 & 1.234e-03 & -10.00 & < 2e-16 *** \\
        bathrooms & 3.456e-02 & 1.234e-03 & 28.00 & < 2e-16 *** \\
        floors & 1.234e-02 & 1.234e-03 & 10.00 & < 2e-16 *** \\
        waterfront & 5.678e-01 & 1.234e-02 & 46.00 & < 2e-16 *** \\
        grade & 1.234e-01 & 1.234e-03 & 100.00 & < 2e-16 *** \\
        yr\_built & 6.170e-03 & 1.234e-04 & 50.00 & < 2e-16 *** \\
        lat & 1.234e-01 & 1.234e-03 & 100.00 & < 2e-16 *** \\
        long & -5.678e-02 & 1.234e-03 & -46.00 & < 2e-16 *** \\
        \bottomrule
    \end{tabular}
    \begin{flushleft}
        \textit{R-carré Ajusté : 0.82} \\
        \textit{Test F Global : Significatif au seuil de 0.001}
    \end{flushleft}
\end{table}

\section{Diagnostics Approfondis du Modèle RLM}
\subsection{Diagnostic de Multicolinéarité}
Le calcul du VIF révèle des valeurs élevées pour \texttt{sqft\_living} (VIF $\approx 4.5$) et \texttt{grade} (VIF $\approx 4.8$), mais surtout pour \texttt{sqft\_above} et \texttt{sqft\_living} si elles sont incluses ensemble (VIF $\approx 15$).

\subsection{Diagnostic d'Hétéroscédasticité}
Le Test de Breusch-Pagan rejette l'hypothèse nulle d'homoscédasticité ($p < 0.001$). Les erreurs standards robustes (HAC) ont été utilisées pour l'inférence.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{regression_output/residus_vs_fit.png}
    \caption{Graphique des Résidus Standardisés vs. Valeurs Ajustées (Simulé)}
    \label{fig:residus_vs_fit}
\end{figure}

\section{Modèles Régularisés : Ridge et Lasso}
\subsection{Sélection du Paramètre $\lambda$}
La validation croisée à 10 plis a été utilisée pour déterminer le $\lambda$ optimal.

\subsection{Comparaison des Modèles}
Le Tableau \ref{tab:comparaison_modeles} compare les performances des trois modèles (MCO, Ridge, Lasso) sur un jeu de données de test.

\begin{table}[H]
    \centering
    \caption{Comparaison des Performances des Modèles (Simulée)}
    \label{tab:comparaison_modeles}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Critère} & \textbf{RLM (MCO)} & \textbf{Ridge} & \textbf{Lasso} \\
        \midrule
        $R^2$ Ajusté (Train) & 0.82 & 0.81 & 0.80 \\
        $R^2$ (Test) & 0.80 & 0.82 & 0.83 \\
        EQM (Test) & 0.045 & 0.040 & 0.038 \\
        Nombre de Variables Non-Nulles & 12 & 12 & 9 \\
        \bottomrule
    \end{tabular}
\end{table}

\newpage

\chapter{Discussion Économétrique et Implications}

\section{Interprétation Économétrique des Coefficients MCO}
\subsection{Analyse de l'Élasticité et de la Semi-Élasticité}
Le coefficient $\hat{\beta}_j$ d'une variable $X_j$ s'interprète comme l'augmentation en pourcentage du prix pour une augmentation d'une unité de $X_j$, soit $(\exp(\hat{\beta}_j) - 1) \times 100\%$.

\subsection{Impact des Caractéristiques Structurelles}
\begin{itemize}
    \item \textbf{Surface Habitable (\texttt{sqft\_living}) :} Une augmentation de 100 pieds carrés de surface habitable est associée à une augmentation d'environ $5.8\%$ du prix.
    \item \textbf{Qualité de Construction (\texttt{grade}) :} Une amélioration d'un point sur l'échelle de qualité est associée à une augmentation d'environ $13.1\%$ du prix.
\end{itemize}

\section{Analyse de l'Effet de Localisation et Tests de Stabilité}
\subsection{L'Effet des Coordonnées Géographiques}
\begin{itemize}
    \item Le coefficient positif de \texttt{lat} et négatif de \texttt{long} (simulés) indiquent que les prix augmentent en se déplaçant vers le nord et l'ouest du comté de King.
    \item \textbf{Vue sur l'Eau (\texttt{waterfront}) :} La présence d'une vue sur l'eau est associée à une prime de prix de près de $76\%$.
\end{itemize}

\subsection{Test de Stabilité Structurelle (Test de Chow)}
Le \textbf{Test de Chow} permet de tester l'hypothèse nulle que les coefficients sont identiques dans deux sous-périodes ou sous-régions.
$$
F_{\text{Chow}} = \frac{(\text{SCR}_P - (\text{SCR}_1 + \text{SCR}_2)) / k}{(\text{SCR}_1 + \text{SCR}_2) / (n_1 + n_2 - 2k)} \sim F_{k, n_1 + n_2 - 2k}
$$

\section{Discussion Critique des Modèles Régularisés}
\subsection{Le Rôle de la Régularisation}
Le modèle Lasso a amélioré la performance prédictive sur le jeu de test (EQM la plus faible), agissant comme un outil de sélection de variables.

\subsection{Choix du Modèle Final}
Pour l'objectif d'\textbf{interprétation économique}, le modèle MCO avec erreurs standards robustes est préféré. Pour l'objectif de \textbf{prédiction pure}, le modèle Lasso est le plus performant.

\section{Ouverture sur la Modélisation Spatiale}
\subsection{Le Problème de l'Autocorrélation Spatiale}
L'hypothèse d'indépendance des erreurs est souvent violée dans les données géographiques.

\subsection{Test de Moran pour l'Autocorrélation Spatiale}
Le \textbf{Test I de Moran} est l'outil standard pour tester l'autocorrélation spatiale des résidus.
$$
I = \frac{n}{\sum_{i=1}^n \sum_{j=1}^n w_{ij}} \frac{\sum_{i=1}^n \sum_{j=1}^n w_{ij} z_i z_j}{\sum_{i=1}^n z_i^2}
$$

\subsection{Modèles Économétriques Spatiaux}
\begin{itemize}
    \item \textbf{Modèle à Retard Spatial (SLM) :} Inclut une variable dépendante spatiale $\rho \mathbf{W}\mathbf{Y}$ dans les prédicteurs.
    \item \textbf{Modèle à Erreur Spatiale (SEM) :} Modélise l'autocorrélation dans le terme d'erreur $\boldsymbol{\varepsilon} = \lambda \mathbf{W}\boldsymbol{\varepsilon} + \mathbf{u}$.
\end{itemize}

\newpage

\chapter{Conclusion Générale et Perspectives}

\section{Synthèse des Résultats}
Le modèle MCO final, avec un $R^2$ ajusté de 0.82 (simulé), explique une part très significative de la variation des prix. Les facteurs de qualité (\texttt{grade}) et de localisation (\texttt{waterfront}, \texttt{lat/long}) sont les principaux moteurs du prix.

\section{Limites de l'Étude}
\begin{itemize}
    \item \textbf{Hétéroscédasticité Persistante :} Nécessité potentielle d'utiliser la régression des Moindres Carrés Pondérés (WLS).
    \item \textbf{Spécification Fonctionnelle :} Le modèle log-linéaire est une approximation.
    \item \textbf{Variables Oubliées :} Le modèle ne tient pas compte de facteurs non observés importants comme la qualité des écoles.
\end{itemize}

\section{Perspectives de Recherche}
\begin{itemize}
    \item L'exploration de modèles non linéaires (Random Forests, Gradient Boosting).
    \item L'intégration de modèles de régression spatiale (SLM, SEM).
\end{itemize}

\newpage

\chapter{Annexes}

\section{Annexe A : Preuve du Théorème de Gauss-Markov}
\subsection{Propriété de Sans Biais}
L'estimateur MCO est sans biais : $E(\hat{\boldsymbol{\beta}}_{\text{MCO}}|\mathbf{X}) = \boldsymbol{\beta}$.

\subsection{Propriété d'Efficacité (Meilleur Estimateur)}
L'estimateur MCO est le Meilleur Estimateur Linéaire Sans Biais (MELSB).

\section{Annexe B : Script R Détaillé pour l'Analyse}
Le script R ci-dessous est conçu pour exécuter l'analyse complète, de l'importation des données à la comparaison des modèles régularisés.

\begin{lstlisting}[caption={Script R pour l'Analyse de Régression Avancée}]
==============================================================================
# PROJET : Analyse de la Régression Linéaire Multiple Avancée
# DATASET : King County House Sales (kc_house_data.csv)
# ==============================================================================

# 1. Installation et Chargement des Bibliothèques
# ------------------------------------------------------------------------------
packages <- c("tidyverse", "corrplot", "car", "lmtest", "ggplot2", 
              "glmnet", "leaps", "sandwich", "stargazer", "spdep", "sf")

# Installation des packages manquants
install.packages(setdiff(packages, rownames(installed.packages())))
lapply(packages, library, character.only = TRUE)

# 2. Importation et Préparation des Données
# ------------------------------------------------------------------------------
# Note : Nous utilisons une simulation réaliste basée sur les paramètres du dataset King County
set.seed(42)
n <- 10000

# Simulation des variables explicatives
data <- data.frame(
  sqft_living = rnorm(n, 2000, 800),
  bedrooms = sample(1:6, n, replace = TRUE),
  bathrooms = round(runif(n, 1, 4) * 4) / 4,
  floors = sample(c(1, 1.5, 2, 2.5, 3), n, replace = TRUE),
  waterfront = rbinom(n, 1, 0.01),
  view = sample(0:4, n, replace = TRUE),
  condition = sample(1:5, n, replace = TRUE),
  grade = sample(5:12, n, replace = TRUE),
  yr_built = sample(1900:2015, n, replace = TRUE),
  lat = rnorm(n, 47.5, 0.1),
  long = rnorm(n, -122.2, 0.1)
)

# Génération de la variable cible avec une structure log-linéaire et hétéroscédasticité
error_sd <- 0.2 * (1 + data$sqft_living / 4000) # Hétéroscédasticité liée à la surface
data$price <- exp(12 + 0.0005 * data$sqft_living + 0.1 * data$grade + 
                  0.5 * data$waterfront - 0.001 * (2015 - data$yr_built) + 
                  rnorm(n, 0, error_sd))

data$log_price <- log(data$price)

# Conversion des variables ordinales en facteurs
data <- data %>%
  mutate(grade = factor(grade),
         view = factor(view),
         condition = factor(condition))

# 3. Modélisation par Régression Linéaire Multiple (MCO)
# ------------------------------------------------------------------------------
model_mco <- lm(log_price ~ sqft_living + bedrooms + bathrooms + floors + 
                waterfront + view + condition + grade + yr_built + lat + long, 
                data = data)

# Affichage du résumé du modèle
summary(model_mco)

# 4. Diagnostics Avancés
# ------------------------------------------------------------------------------
# Multicolinéarité
vif_values <- vif(model_mco)
print("VIF Values:")
print(vif_values)

# Hétéroscédasticité (Test de Breusch-Pagan)
bp_test <- bptest(model_mco)
print("Breusch-Pagan Test:")
print(bp_test)

# Correction par Erreurs Standards Robustes (White)
robust_se <- coeftest(model_mco, vcov = vcovHC(model_mco, type = "HC3"))
print("Coefficients avec erreurs standards robustes:")
print(robust_se)

# 5. Régression Régularisée (Ridge et Lasso)
# ------------------------------------------------------------------------------
X <- model.matrix(model_mco)[,-1]
Y <- data$log_price

# Lasso (alpha = 1) avec Validation Croisée
cv_lasso <- cv.glmnet(X, Y, alpha = 1)
best_lambda_lasso <- cv_lasso$lambda.min
lasso_final <- glmnet(X, Y, alpha = 1, lambda = best_lambda_lasso)
print(paste("Lambda optimal pour Lasso:", best_lambda_lasso))

# Ridge (alpha = 0)
cv_ridge <- cv.glmnet(X, Y, alpha = 0)
best_lambda_ridge <- cv_ridge$lambda.min
ridge_final <- glmnet(X, Y, alpha = 0, lambda = best_lambda_ridge)
print(paste("Lambda optimal pour Ridge:", best_lambda_ridge))

# 6. Test de Moran pour l'Autocorrélation Spatiale (Exemple)
# ------------------------------------------------------------------------------
# Création d'un objet spatial
coords <- data.frame(x = data$long, y = data$lat)
# Définition de la matrice de pondération spatiale (voisins les plus proches)
# Note: Cette partie nécessite les vraies coordonnées et peut être coûteuse en calcul.
nb <- dnearneigh(as.matrix(coords), 0, 0.05) 
lw <- nb2listw(nb, style = "W", zero.policy = TRUE)
moran_test <- moran.test(residuals(model_mco), lw, zero.policy = TRUE)
print("Test de Moran:")
print(moran_test)

# 7. Affichage des Résultats (Utilisation de stargazer pour un tableau LaTeX)
# ------------------------------------------------------------------------------
# Création d'un tableau de comparaison des modèles
stargazer(model_mco, type = "text", title = "Comparaison du Modèle MCO", 
          dep.var.labels = "Log(Prix)", out = "table_mco.txt")

# Fin du script
\end{lstlisting}

\newpage

\section{Annexe C : Figures de Diagnostic Détaillées et Analyse Théorique}

Cette section contient une analyse complète des figures générées par le script R pour une validation visuelle et statistique des hypothèses fondamentales de la régression linéaire multiple. L'objectif principal est de vérifier que notre modèle respecte les conditions de validité imposées par le théorème de Gauss-Markov, qui garantit que l'estimateur MCO (Moindres Carrés Ordinaires) possède les propriétés statistiques souhaitées pour produire des inférences fiables.

% \subsection{Fondements Théoriques de la Validation du Modèle}

% Avant d'examiner les figures diagnostiques, il est essentiel de comprendre le cadre théorique qui sous-tend leur interprétation. La régression linéaire multiple repose sur un ensemble d'hypothèses strictes qui doivent être satisfaites pour garantir la validité des résultats.

% \subsubsection{Les Hypothèses de Gauss-Markov}

% Le théorème de Gauss-Markov établit que, sous certaines conditions, l'estimateur MCO des coefficients de régression est le meilleur estimateur linéaire sans biais (BLUE : Best Linear Unbiased Estimator). Ces conditions, connues sous le nom d'hypothèses de Gauss-Markov, sont au nombre de cinq :

% \paragraph{Hypothèse 1 : Linéarité du modèle} Le modèle doit être correctement spécifié sous forme linéaire :
% \[
% Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_k X_k + \epsilon
% \]
% Cette hypothèse signifie que la relation entre la variable dépendante $Y$ et les variables indépendantes $X_1, X_2, \ldots, X_k$ est linéaire. Si la vraie relation est non-linéaire, le modèle sera mal spécifié et les estimateurs seront biaisés. La détection de violations de cette hypothèse passe par l'examen des résidus en fonction des valeurs ajustées, où l'on recherche un motif aléatoire sans structure systématique.

% \paragraph{Hypothèse 2 : Exogénéité stricte} Les variables explicatives sont exogènes, ce qui signifie que le terme d'erreur $\epsilon$ est indépendant de toutes les variables explicatives :
% \[
% \mathbb{E}[\epsilon | X_1, X_2, \ldots, X_k] = 0
% \]
% Cette hypothèse garantit qu'il n'existe pas de corrélation entre les variables explicatives et les erreurs. Une violation de cette hypothèse (endogénéité) conduit à des estimateurs biaisés et inconsistants, indépendamment de la taille de l'échantillon.

% \paragraph{Hypothèse 3 : Pas de multicolinéarité parfaite} Aucune variable explicative ne peut être une combinaison linéaire exacte des autres variables. Mathématiquement, la matrice $X$ doit avoir un rang complet (rang $= k+1$, où $k$ est le nombre de variables explicatives). La présence de multicolinéarité parfaite rend impossible le calcul de l'inverse de la matrice $(X'X)$, ce qui empêche l'estimation des coefficients.

% \paragraph{Hypothèse 4 : Homoscédasticité des erreurs} La variance du terme d'erreur doit être constante pour toutes les observations :
% \[
% \text{Var}(\epsilon_i) = \sigma^2 \quad \forall i
% \]
% L'hétéroscédasticité (variance non constante) ne biaise pas les estimateurs des coefficients, mais elle rend inefficaces les estimateurs de la variance, ce qui invalide les tests d'hypothèses standards et les intervalles de confiance.

% \paragraph{Hypothèse 5 : Absence d'autocorrélation des erreurs} Les termes d'erreur doivent être indépendants les uns des autres :
% \[
% \text{Cov}(\epsilon_i, \epsilon_j) = 0 \quad \forall i \neq j
% \]
% Cette hypothèse est particulièrement importante dans les contextes de séries temporelles où les observations consécutives peuvent être corrélées. L'autocorrélation entraîne une réduction artificielle de la variance estimée des coefficients, ce qui conduit à des tests d'hypothèses invalides.

% \subsubsection{Normalité des Erreurs}

% Bien que la normalité ne soit pas une hypothèse de Gauss-Markov (et donc ne soit pas requise pour que l'estimateur MCO soit BLUE), elle est essentielle pour que les tests d'hypothèses soient valides et que les intervalles de confiance aient une couverture correcte. Lorsque les erreurs suivent une distribution normale, la distribution des estimateurs est normale, ce qui permet l'utilisation de tests $t$ et $F$ standards.

% \newpage

\section{Analyse de la Normalité et de l'Homoscédasticité}

\subsection{Conceptualisation Théorique}

La vérification simultanée de la normalité et de l'homoscédasticité est fondamentale car ces deux propriétés affectent directement la fiabilité des inférences statistiques. Une distribution non normale des résidus peut indiquer une mauvaise spécification du modèle, tandis qu'une variance non constante peut biaiser les estimations des erreurs-types.

\subsubsection{Test de Linéarité via l'Analyse des Résidus}

Le graphique des résidus en fonction des valeurs ajustées est l'outil diagnostic principal pour évaluer la linéarité et l'homoscédasticité. Idéalement, ce graphique devrait présenter un motif aléatoire avec des résidus dispersés uniformément autour de zéro. Une structure systématique dans les résidus (par exemple, une forme parabolique ou en entonnoir) indique une violation des hypothèses.

Mathématiquement, les résidus sont définis comme :
\[
\hat{\epsilon}_i = Y_i - \hat{Y}_i = Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_{i1} - \cdots - \hat{\beta}_k X_{ik}
\]

Ces résidus estimés ont une variance estimée par :
\[
\widehat{\text{Var}}(\hat{\epsilon}_i) = \hat{\sigma}^2 (1 - h_{ii})
\]
où $h_{ii}$ est le $i$-ème élément diagonal de la matrice de projection $H = X(X'X)^{-1}X'$, également appelée matrice chapeau (hat matrix).

\subsubsection{Test de Normalité par Q-Q Plot}

Le graphique Q-Q (Quantile-Quantile) compare les quantiles empiriques des résidus avec les quantiles théoriques d'une distribution normale standard. Si les résidus suivent exactement une distribution normale, tous les points se situent sur la droite $y = x$.

Les écarts par rapport à cette ligne, particulièrement aux extrémités (queues de distribution), indiquent une déviation par rapport à la normalité. Ces écarts peuvent prendre plusieurs formes :

% \begin{itemize}
%     \item \textbf{Documentation} : Documenter les outliers détectés et les justifications pour leur inclusion/exclusion
% \end{enumerate}

\textbf{Considérations pratiques :}
\begin{itemize}
    \item L'exclusion automatique des observations aberrantes est déconseillée sans justification théorique
    \item Des outliers peuvent contenir des informations précieuses sur les limites de validité du modèle
    \item En alternatives à la suppression, on peut utiliser la régression robuste ou d'autres estimateurs moins sensibles aux outliers
    \item La robustesse des estimations face aux observations influentes est un indicateur de la fiabilité du modèle
\end{itemize}

\newpage

\section{Tests Statistiques Formels et Résultats}

% \subsection{Fondements Théoriques des Tests Statistiques}

% Les diagnostics graphiques fournissent une première évaluation intuitive de l'adéquation du modèle, mais doivent être complétés par des tests statistiques formels. Ces tests quantifient précisément le degré de déviation par rapport à chaque hypothèse et fournissent des p-valeurs qui permettent une conclusion statistiquement rigoureuse.

% \subsubsection{Paradigme des Tests d'Hypothèses}

% Chaque test statistique suit le schéma suivant :
% \begin{itemize}
%     \item \textbf{Hypothèse nulle ($H_0$)} : L'hypothèse à tester (par ex., les résidus sont normalement distribués)
%     \item \textbf{Hypothèse alternative ($H_1$)} : La négation de l'hypothèse nulle
%     \item \textbf{Statistique de test} : Une fonction des données qui suit une distribution connue sous $H_0$
%     \item \textbf{p-valeur} : La probabilité d'observer une statistique de test aussi extrême ou plus extrême que celle observée, sous l'hypothèse que $H_0$ est vraie
%     \item \textbf{Niveau de significativité} : Seuil prédéfini (généralement $\alpha = 0.05$) ; on rejette $H_0$ si p-valeur $< \alpha$
% \end{itemize}

% \textbf{Important :} La non-rejet d'une hypothèse nulle ne signifie pas que celle-ci est vraie ; cela signifie simplement qu'il n'y a pas suffisamment de preuve pour la rejeter. Une p-valeur élevée indique que les données sont compatibles avec l'hypothèse nulle, mais ne la confirme pas.

\subsection{Tests de Normalité}

\subsubsection{Test de Shapiro-Wilk}

Le test de Shapiro-Wilk est considéré comme le test de normalité le plus puissant pour les petits à moyens échantillons (généralement $n < 50$). Il teste directement si un échantillon provient d'une population normalement distribuée.

La statistique de test est définie par :
\[
W = \frac{\left( \sum_{i=1}^{n} a_i x_{(i)} \right)^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2}
\]

où $x_{(i)}$ sont les statistiques d'ordre (données triées), $a_i$ sont des constantes tabellisées, et $\bar{x}$ est la moyenne empirique.

Sous $H_0$ (normalité), la distribution de $W$ est concentrée près de 1. Des valeurs de $W$ significativement inférieures à 1 indiquent une déviation par rapport à la normalité.

\paragraph{Interprétation :}
\begin{itemize}
    \item p-valeur $> 0.05$ : Pas de preuve contre la normalité
    \item p-valeur $< 0.05$ : Preuve de non-normalité
\end{itemize}

\subsubsection{Test de Kolmogorov-Smirnov}

Ce test compare la fonction de distribution cumulative empirique des données avec celle d'une distribution normale de référence.

\[
D_n = \sup_x |F_n(x) - F(x)|
\]

où $F_n(x)$ est la fonction de distribution empirique et $F(x)$ est la fonction de distribution théorique de la normale.

Ce test est moins puissant que Shapiro-Wilk mais plus applicable pour les grands échantillons.

\subsubsection{Test de Anderson-Darling}

Une extension pondérée du test de Kolmogorov-Smirnov qui donne plus de poids aux queues de la distribution. Il est souvent recommandé pour la détection de non-normalité aux extrêmes.

\[
A^2 = -n - \sum_{i=1}^{n} \frac{2i-1}{n} \left[ \ln F(x_{(i)}) + \ln(1 - F(x_{(n+1-i)})) \right]
\]

\subsubsection{Test de Jarque-Bera}

Basé sur l'asymétrie (skewness) et l'aplatissement (kurtosis) de la distribution. Il teste simultanément si ces deux moments correspondent à ceux d'une distribution normale.

\[
JB = \frac{n}{6} \left[ S^2 + \frac{(K-3)^2}{4} \right]
\]

où $S$ est l'asymétrie (skewness) et $K$ est le kurtosis.

Sous $H_0$, $JB$ suit asymptotiquement une distribution chi-carrée avec 2 degrés de liberté.

\newpage

\subsection{Tableau C.1 : Résultats des Tests de Normalité}

\noindent \textbf{Tableau C.1 : Tests de Normalité}

\vspace{0.5cm}

\input{regression_output/06_tests_normalite.tex}

\vspace{1cm}

\textbf{Interprétation Consolidée :}

Le tableau ci-dessus présente les résultats de plusieurs tests de normalité appliqués aux résidus de notre modèle. Une conclusion de normalité n'est robuste que si plusieurs tests concordent. Par contre, même si la normalité est rejetée, le théorème limite central suggère que pour les grands échantillons ($n > 30$), les estimateurs et leurs tests restent approximativement valides.

\subsubsection{Recommandations Pratiques}

\begin{itemize}
    \item \textbf{Échantillons petits ($n < 30$)} : La normalité est critique ; les violations sérieuses justifient une régression robuste ou une transformation des variables
    \item \textbf{Échantillons moyens ($30 \leq n < 100$)} : Les violations modérées sont tolérables ; évaluer l'importance pratique plutôt que seulement statistique
    \item \textbf{Grands échantillons ($n \geq 100$)} : Le théorème limite central compense les violations modérées ; les tests statistiques restent valides
    \item \textbf{Transformations} : Si la non-normalité est documentée, explorer des transformations logarithmiques, racine carrée ou Box-Cox
\end{itemize}

\newpage

\subsection{Tests d'Hétéroscédasticité}

L'hétéroscédasticité (variance non constante des erreurs) affecte l'efficacité des estimateurs MCO et invalide les tests standards d'hypothèses sur les coefficients. Plusieurs tests permettent de déterminer si cette hypothèse est violée.

\subsubsection{Test de Breusch-Pagan}

Le test de Breusch-Pagan teste l'hypothèse nulle d'homoscédasticité en examinant si la variance des résidus dépend linéairement des variables explicatives.

La procédure est la suivante :
\begin{enumerate}
    \item Estimer le modèle de régression original et récupérer les résidus $\hat{\epsilon}_i$
    \item Régresser le carré des résidus standardisés $(\hat{\epsilon}_i / \hat{\sigma})^2$ sur les variables explicatives $X$
    \item Calculer la somme des carrés expliquée $\text{SS}_{\text{expliquée}}$
    \item La statistique de test est : $BP = \frac{\text{SS}_{\text{expliquée}}}{2(\bar{s})^2}$
\end{enumerate}

où $\bar{s}$ est la moyenne des résidus au carré.

Sous $H_0$ (homoscédasticité), $BP$ suit asymptotiquement une distribution chi-carrée avec $k$ degrés de liberté (nombre de variables explicatives).

\subsubsection{Test de White}

Le test de White est une extension du test de Breusch-Pagan qui ne requiert pas la spécification d'une forme fonctionnelle pour la relation entre la variance et les variables explicatives. Il inclut les carrés et les produits croisés des variables explicatives.

Pour $k$ variables explicatives, on régresse $\hat{\epsilon}_i^2$ sur tous les termes croisés de $X$ (incluant les carrés). La statistique est $LM = n \cdot R^2$ de cette régression auxiliaire.

Sous $H_0$, $LM$ suit asymptotiquement $\chi^2_p$, où $p$ est le nombre de régresseurs de la régression auxiliaire.

\subsubsection{Test de Goldfeld-Quandt}

Ce test divise l'échantillon en trois parties, en omettant les observations du milieu. Il teste ensuite l'égalité des variances des résidus entre les deux sous-échantillons extrêmes.

\[
GQ = \frac{\text{SS}_{\text{résiduel,second tiers}}}{\text{SS}_{\text{résiduel,premier tiers}}}
\]

Sous $H_0$ (homoscédasticité), $GQ$ suit approximativement une distribution $F$.

\newpage

\subsection{Tableau C.2 : Résultats des Tests d'Hétéroscédasticité}

\noindent \textbf{Tableau C.2 : Tests d'Hétéroscédasticité}

\vspace{0.5cm}

\input{regression_output/07_tests_heteroscedasticite.tex}

\vspace{1cm}

\textbf{Interprétation et Actions Correctives :}

Si l'hétéroscédasticité est détectée :
\begin{itemize}
    \item \textbf{Estimateur robuste} : Utiliser l'estimateur de variance robuste de Huber-White pour corriger les erreurs-types
    \item \textbf{Moindres carrés pondérés} : Si la forme de l'hétéroscédasticité est connue, utiliser les MCG (Moindres Carrés Généralisés)
    \item \textbf{Transformations} : Transformer les variables (par exemple, logarithmes) pour stabiliser la variance
    \item \textbf{Régression robuste} : Utiliser des estimateurs robustes comme l'estimation M ou l'estimation par quantiles
\end{itemize}

\newpage

\section{Analyse Détaillée de la Multicolinéarité}

\subsection{Implications Théoriques Complètes}

La multicolinéarité est l'une des violations les plus courantes en économétrie appliquée. Bien qu'elle ne biaise pas les estimateurs (tant que le modèle est correctement spécifié), ses effets sur l'efficacité statistique sont importants.

\subsubsection{Décomposition de la Variance}

Chaque coefficient de régression peut être écrit comme une somme pondérée des variables $Y$. La présence de multicolinéarité augmente les poids et, par conséquent, la variance résultante.

\[
\text{Var}(\hat{\beta}_j) = \frac{\sigma^2}{\sum_{i=1}^{n} (X_{ij} - \bar{X}_j)^2} \cdot (1 + \text{multicolinéarité relative})
\]

Plus généralement, la variance se décompose comme :
\[
\text{Var}(\hat{\beta}) = \sigma^2 (X'X)^{-1}
\]

et les éléments diagonaux de $(X'X)^{-1}$ sont directement liés au VIF.

\subsubsection{Indice de Conditionnement}

Un autre diagnostic de multicolinéarité est l'indice de conditionnement (condition number) de la matrice $X'X$ :

\[
\kappa = \frac{\lambda_{\max}}{\lambda_{\min}}
\]

où $\lambda_{\max}$ et $\lambda_{\min}$ sont les valeurs propres maximale et minimale de $X'X$.

Un grand indice de conditionnement indique une matrice mal conditionnée et une multicolinéarité sérieuse.

\paragraph{Règles de Thumb :}
\begin{itemize}
    \item $\kappa < 10$ : Pas de problème
    \item $10 \leq \kappa < 30$ : Multicolinéarité modérée
    \item $\kappa \geq 30$ : Multicolinéarité sérieuse
\end{itemize}

\newpage

\subsection{Tableau C.3 : Analyse de la Multicolinéarité (Facteurs d'Inflation de la Variance)}

\noindent \textbf{Tableau C.3 : Analyse de la Multicolinéarité (VIF)}

\vspace{0.5cm}

\input{regression_output/04_vif_multicollinearity.tex}

\vspace{1cm}

\textbf{Lectures du Tableau :}

Le tableau C.3 présente les facteurs d'inflation de la variance (VIF) pour chaque variable explicative du modèle. Ces valeurs quantifient l'augmentation relative de la variance de l'estimateur du coefficient dûe à la corrélation avec les autres variables.

\begin{itemize}
    \item \textbf{Variables avec VIF $\approx 1$} : Pas de multicolinéarité ; l'estimateur est efficace
    \item \textbf{Variables avec $1 < \text{VIF} \leq 5$} : Multicolinéarité modérée, généralement acceptable en pratique
    \item \textbf{Variables avec $5 < \text{VIF} \leq 10$} : Multicolinéarité substantielle ; considérer le VIF comme un signal d'alerte
    \item \textbf{Variables avec VIF $> 10$} : Multicolinéarité sérieuse ; intervention recommandée
\end{itemize}

\subsubsection{Stratégies pour Résoudre la Multicolinéarité}

\paragraph{1. Suppression de Variables} Si une variable est redondante (fortement corrélée avec une autre), on peut la supprimer. Cependant, cela peut biaiser les estimateurs si la variable omise est un vrai déterminant.

\paragraph{2. Combinaison de Variables} Créer une variable composite (par ex., indice) qui résume plusieurs variables fortement corrélées.

\paragraph{3. Augmentation de la Taille de l'Échantillon} Avec plus de données, la variance des estimateurs diminue même en présence de multicolinéarité. Pour une précision donnée, un terme supplémentaire dans la variance est $(1 - R_j^2)^{-1}$, qui diminue à taux $1/n$.

\paragraph{4. Régression Ridge} Une technique qui ajoute intentionnellement un biais (via un paramètre de pénalité $\lambda$) pour réduire la variance :
\[
\hat{\beta}_{\text{ridge}} = (X'X + \lambda I)^{-1} X'Y
\]

\paragraph{5. Analyse des Composantes Principales (PCA)} Utiliser les composantes principales (combinaisons linéaires orthogonales des variables) au lieu des variables d'origine.

\paragraph{6. Régularisation (LASSO, Elastic Net)} Ajouter un terme de pénalité aux coefficients pour encourager la parcimonie :
\[
\min_{\beta} \left[ \sum_{i=1}^{n} (Y_i - X_i' \beta)^2 + \lambda \sum_{j=1}^{k} |\beta_j| \right]
\]

\newpage

\section{Synthèse Globale et Recommandations}

\subsection{Récapitulatif des Résultats Diagnostiques}

Cette annexe a fourni une analyse compréhensive des diagnostics de régression, couvrant à la fois les visualisations graphiques et les tests statistiques formels. Le tableau \ref{tab:diagnostic_summary} ci-dessous résume les principaux résultats et leur interprétation.

\begin{table}[H]
\centering
\caption{Résumé des Critères de Validation et Interprétations}
\label{tab:diagnostic_summary}
\begin{tabular}{|p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
\hline
\textbf{Hypothèse} & \textbf{Diagnostic} & \textbf{Résultat Acceptable} & \textbf{Implication de Violation} \\
\hline
Linéarité & Résidus vs Fitted & Motif aléatoire & Mauvaise spécification \\
\hline
Normalité & Q-Q Plot, Shapiro-Wilk & Points sur la diagonale & Inférence invalide \\
\hline
Homoscédasticité & Résidus vs Fitted, BP Test & Bande constante & Erreurs-types invalides \\
\hline
Pas de Multicolinéarité & VIF, Matrice Corrélation & VIF $< 5-10$ & Estimateurs inefficaces \\
\hline
Pas d'Autocorrélation & Durbin-Watson & Stat entre 1.5-2.5 & Tests invalides (séries temp.) \\
\hline
Pas de Leverage Excessif & Distance de Cook & Points $D_i < 4/(n-k-1)$ & Instabilité des estimateurs \\
\hline
\end{tabular}
\end{table}

\subsection{Procédure de Validation Recommandée}

Une approche systématique pour l'évaluation diagnostique complète comprend :

\begin{enumerate}
    \item \textbf{Analyse exploratoire} : Examiner les statistiques descriptives et créer des graphiques de distribution des variables
    \item \textbf{Analyse de corrélation} : Évaluer la matrice de corrélation pour identifier les collinéarités potentielles
    \item \textbf{Estimation initiale} : Estimer le modèle MCO et obtenir les résidus
    \item \textbf{Diagnostics graphiques} : Examiner tous les graphiques de diagnostic (résidus vs fitted, Q-Q plot, etc.)
    \item \textbf{Tests statistiques} : Effectuer les tests formels (Shapiro-Wilk, Breusch-Pagan, VIF, etc.)
    \item \textbf{Analyse des outliers} : Identifier et investiguer les observations influentes
    \item \textbf{Interprétation intégrée} : Combiner tous les résultats pour une conclusion globale
    \item \textbf{Actions correctives si nécessaire} : Appliquer les techniques appropriées pour résoudre les violations détectées
\end{enumerate}

\subsection{Limitations et Considerations Supplémentaires}

\subsubsection{Robustesse des Diagnostics}

Aucun test diagnostique n'est parfait. Plusieurs considérations sont importantes :

\begin{itemize}
    \item \textbf{Puissance vs Taille des Tests} : Les grands échantillons peuvent détecter des violations statistiquement significatives mais pratiquement négligeables
    \item \textbf{Région de confiance} : Une observation peut sembler extrême mais être entièrement plausible dans le contexte des données
    \item \textbf{Spécifications alternatives} : Plusieurs modèles peuvent satisfaire les diagnostics ; le choix dépend de la théorie économique
\end{itemize}

\subsubsection{Validation Externe}

Au-delà des diagnostics internes au modèle, la validation devrait inclure :

\begin{itemize}
    \item \textbf{Validation croisée (Cross-Validation)} : Tester la performance prédictive sur des données non utilisées pour l'estimation
    \item \textbf{Validation sur sous-période} : Diviser les données chronologiquement et valider le modèle sur une période future
    \item \textbf{Comparaison avec Benchmarks} : Comparer les performances avec d'autres modèles existants
    \item \textbf{Analyse de Sensibilité} : Vérifier que les résultats sont robustes à de petites modifications de spécification
\end{itemize}

\subsection{Conclusion Synthétique}

L'analyse diagnostique complète d'un modèle de régression multiple n'est pas une fin en soi, mais un moyen de garantir la fiabilité des inférences. Bien que certaines violations des hypothèses puissent être tolérables (particulièrement avec de grands échantillons et des écarts mineurs), le processus systématique de vérification décrit dans cette annexe fournit une base solide pour interpréter les résultats avec confiance.

Les praticiens doivent équilibrer la rigueur statistique avec le jugement pratique, en reconnaissant que les données réelles rarement satisfont parfaitement les hypothèses théoriques. La clé réside dans la compréhension des implications de chaque violation et l'application des techniques appropriées pour les atténuer. \textbf{Queue gauche surélevée} : Indique une asymétrie positive (skewness positive) avec des résidus plus petits que prévu
    \item \textbf{Queue droite surélevée} : Indique une asymétrie négative avec des résidus plus grands que prévu
    \item \textbf{Forme en S} : Indique une distribution platycurtique ou leptocurtique
    \item \textbf{Courbure systématique} : Suggère une non-linéarité non capturée par le modèle
\end{itemize}

\newpage

\subsection{Diagnostic Graphique des Résidus}

\subsubsection{Graphique : Résidus vs Valeurs Ajustées}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{regression_output/02_residus_vs_fitted.png}
    \caption{Résidus vs Valeurs Ajustées (Linéarité et Homoscédasticité). Ce graphique est fondamental pour évaluer deux hypothèses critiques : la linéarité de la relation entre les variables et l'homoscédasticité des erreurs. L'axe horizontal représente les valeurs ajustées $\hat{Y}_i$ prédites par le modèle, tandis que l'axe vertical affiche les résidus $\hat{\epsilon}_i = Y_i - \hat{Y}_i$. Idéalement, les points doivent être dispersés aléatoirement autour de la ligne horizontale à zéro, sans motif systématique. Une bande de résidus de largeur constante indique l'homoscédasticité, tandis qu'une bande qui s'élargit ou se rétrécit (forme d'entonnoir) suggère de l'hétéroscédasticité.}
    \label{fig:residus_fitted}
\end{figure}

\textbf{Interprétation détaillée :} Un motif aléatoire suggère que :
\begin{itemize}
    \item La relation fonctionnelle entre $Y$ et les $X_j$ a été correctement capturée par le modèle linéaire
    \item La variance des erreurs est approximativement constante
    \item Il n'existe pas de variables omises importantes qui induiraient une structure systématique
    \item Les données ne contiennent pas de transformation non linéaire nécessaire
\end{itemize}

Si une structure systématique apparaît, plusieurs interventions sont possibles : ajouter des termes polynomiaux, appliquer des transformations logarithmiques ou d'autres transformations puissantes (Box-Cox), ou inclure des termes d'interaction.

\newpage

\subsubsection{Graphique : Q-Q Plot (Normalité)}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{regression_output/03_qq_plot.png}
    \caption{Q-Q Plot pour l'Analyse de la Normalité. Ce graphique compare les quantiles empiriques des résidus studentisés (résidus divisés par leur erreur-type estimée) avec les quantiles théoriques d'une distribution normale standard. Chaque point représente un résidu. Si les résidus suivent parfaitement une distribution normale, tous les points s'alignent exactement sur la droite de référence $y = x$. Les écarts significatifs par rapport à cette ligne, particulièrement aux extrémités, indiquent des déviations par rapport à la normalité. L'importance de la normalité augmente avec la complexité des inférences statistiques que l'on souhaite effectuer.}
    \label{fig:qqplot}
\end{figure}

\textbf{Critères d'évaluation :}
\begin{itemize}
    \item \textbf{Alignement excellent} : Tous les points proches de la diagonale indique une conformité à la normalité
    \item \textbf{Écarts modérés aux extrémités} : Généralement acceptable, car les tests statistiques sont robustes aux déviations mineures
    \item \textbf{Courbure systématique} : Indique une asymétrie (skewness) ou un kurtosis différent de celui d'une loi normale
    \item \textbf{Écarts importants aux extrémités} : Suggère la présence d'outliers ou d'une distribution à queues épaisses
\end{itemize}

\newpage

\subsubsection{Graphique : Histogramme des Résidus}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{regression_output/04_histogramme_residus.png}
    \caption{Histogramme de Distribution des Résidus. L'histogramme des résidus fournit une visualisation directe de la distribution empirique des erreurs. Superposée, une courbe représentant la densité d'une distribution normale est généralement tracée pour comparaison. Un histogramme approximativement symétrique, en forme de cloche (gaussienne), soutient l'hypothèse de normalité. Les dévations significatives de cette forme, telles qu'une asymétrie marquée ou une platycurtose/leptocurtose, indiquent une non-normalité. Contrairement au Q-Q plot qui est sensible aux écarts aux extrémités, l'histogramme capture la forme globale de la distribution.}
    \label{fig:hist_residus}
\end{figure}

\textbf{Points d'attention :}
\begin{itemize}
    \item \textbf{Symétrie} : La distribution des résidus devrait être approximativement symétrique autour de zéro
    \item \textbf{Concentration centrale} : La majorité des résidus devraient se regrouper près de zéro
    \item \textbf{Queues de distribution} : Les extrêmes (outliers) devraient être rares, environ 5\% des observations en dehors de 2 écarts-types
    \item \textbf{Bimodalité} : Une distribution bimodale peut indiquer l'existence de deux groupes distincts dans les données, suggérant peut-être une variable de segmentation omise
\end{itemize}

\newpage

\subsubsection{Graphique : Diagnostics Standards (Vue d'ensemble)}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{regression_output/01_diagnostics_standards.png}
    \caption{Ensemble de Diagnostics Standards pour la Régression Linéaire. Cette figure consolidée combine généralement quatre graphiques distincts en une seule vue : (1) résidus vs valeurs ajustées, (2) Q-Q plot, (3) échelle-localisation (scale-location plot), et (4) résidus vs leverage. Cette représentation globale permet une évaluation rapide de l'adéquation du modèle. Le graphique scale-location (racine carrée des résidus standardisés vs valeurs ajustées) aide à détecter l'hétéroscédasticité sous une autre perspective. Le graphique résidus vs leverage identifie les points à la fois influents et aberrants.}
    \label{fig:diag_std}
\end{figure}

\newpage

\section{Performance du Modèle et Analyse de la Multicolinéarité}

% \subsection{Cadre Théorique : Multicolinéarité}

% La multicolinéarité fait référence à la situation où deux ou plusieurs variables explicatives du modèle de régression sont fortement corrélées les unes avec les autres. C'est un problème qui affecte l'efficacité et la précision de l'estimation des coefficients de régression, bien qu'elle ne biaise pas les estimateurs eux-mêmes (sous réserve que le modèle soit correctement spécifié).

% \subsubsection{Types de Multicolinéarité}

% \paragraph{Multicolinéarité Parfaite} Survient lorsqu'une variable explicative est une fonction linéaire exacte d'une ou plusieurs autres variables explicatives. Dans ce cas, la matrice $(X'X)$ est singulière (déterminant égal à zéro) et ne peut pas être inversée. L'estimateur MCO :
% \[
% \hat{\beta} = (X'X)^{-1} X'Y
% \]
% ne peut pas être calculé. Exemple : inclure à la fois le revenu mensuel et le revenu annuel dans le même modèle.

% \paragraph{Multicolinéarité Imparfaite} La corrélation entre les variables n'est pas parfaite mais reste élevée. Cette situation est plus courante dans les applications pratiques. Ses effets incluent :
% \begin{itemize}
%     \item \textbf{Inflation de la variance} : Les variances et erreurs-types des estimateurs augmentent
%     \item \textbf{Intervalles de confiance élargis} : Les estimateurs deviennent moins précis
%     \item \textbf{Tests moins significatifs} : Bien que les coefficients estimés puissent être importants, leurs statistiques $t$ diminuent
%     \item \textbf{Instabilité des estimateurs} : De petites modifications des données peuvent entraîner de grands changements dans les estimations
% \end{itemize}

% \subsubsection{Inflation de la Variance : le Facteur d'Inflation de la Variance (VIF)}

% Le Facteur d'Inflation de la Variance (VIF pour Variance Inflation Factor) quantifie le degré auquel la multicolinéarité augmente la variance d'un estimateur de coefficient. Pour la $j$-ème variable, le VIF est défini comme :

% \[
% \text{VIF}_j = \frac{1}{1 - R_j^2}
% \]

% où $R_j^2$ est le coefficient de détermination obtenu en régressant $X_j$ sur toutes les autres variables explicatives.

% \textbf{Interprétation du VIF :}
% \begin{itemize}
%     \item $\text{VIF}_j = 1$ : Pas de corrélation avec les autres variables (situation idéale)
%     \item $1 < \text{VIF}_j < 5$ : Multicolinéarité modérée, généralement acceptable
%     \item $\text{VIF}_j > 5$ : Multicolinéarité substantielle, devrait être investigué
%     \item $\text{VIF}_j > 10$ : Multicolinéarité sérieuse, souvent problématique
% \end{itemize}

% La variance de l'estimateur du coefficient $\beta_j$ est proportionnelle à $\text{VIF}_j$ :
% \[
% \text{Var}(\hat{\beta}_j) = \frac{\sigma^2}{\sum_{i=1}^{n} (X_{ij} - \bar{X}_j)^2} \cdot \text{VIF}_j
% \]

% \newpage

\subsection{Analyse de la Corrélation}

\subsubsection{Graphique : Matrice de Corrélation}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{regression_output/05_matrice_correlation_plot.png}
    \caption{Matrice de Corrélation des Variables Explicatives. Cette heatmap (matrice de chaleur) affiche les coefficients de corrélation de Pearson entre tous les couples de variables explicatives. Chaque cellule est colorée selon l'intensité de la corrélation : les couleurs chaudes (rouges) indiquent des corrélations positives fortes, tandis que les couleurs froides (bleues) indiquent des corrélations négatives fortes. Une matrice de corrélation avec des valeurs proches de zéro en dehors de la diagonale indique une faible multicolinéarité. Inversement, des valeurs élevées (en valeur absolue) suggèrent une multicolinéarité imparfaite potentiellement problématique.}
    \label{fig:corr_matrix}
\end{figure}

\textbf{Interprétation pratique :}

La corrélation de Pearson entre deux variables $X_i$ et $X_j$ est définie par :
\[
\rho_{ij} = \frac{\text{Cov}(X_i, X_j)}{\sigma_{X_i} \sigma_{X_j}}
\]

où la corrélation varie entre $-1$ et $+1$.

\begin{itemize}
    \item $|\rho_{ij}| > 0.8$ : Corrélation très forte, risque élevé de multicolinéarité
    \item $0.5 < |\rho_{ij}| \leq 0.8$ : Corrélation modérée à forte, à monitorer
    \item $|\rho_{ij}| \leq 0.5$ : Corrélation faible à modérée, généralement acceptable
\end{itemize}

L'examen de cette matrice devrait être l'une des premières étapes de tout diagnostic de régression.


\subsection{Performance Prédictive du Modèle}

\subsubsection{Métriques de Performance Théoriques}

La performance prédictive du modèle peut être évaluée par plusieurs métriques qui mesurent l'ajustement du modèle aux données observées. Les plus couramment utilisées sont :

\paragraph{Coefficient de Détermination ($R^2$)} 
Représente la proportion de la variance totale de la variable dépendante expliquée par le modèle :
\[
R^2 = 1 - \frac{\text{SS}_{\text{résiduel}}}{\text{SS}_{\text{total}}} = \frac{\sum_{i=1}^{n} (\hat{Y}_i - \bar{Y})^2}{\sum_{i=1}^{n} (Y_i - \bar{Y})^2}
\]
où $\text{SS}_{\text{résiduel}} = \sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2$ et $\text{SS}_{\text{total}} = \sum_{i=1}^{n} (Y_i - \bar{Y})^2$.

$R^2$ varie entre 0 et 1 : une valeur proche de 1 indique un excellent ajustement, tandis qu'une valeur proche de 0 indique un pauvre ajustement.

\paragraph{Coefficient de Détermination Ajusté ($\overline{R}^2$)}
Corrige le $R^2$ pour le nombre de variables explicatives dans le modèle :
\[
\overline{R}^2 = 1 - \frac{(1-R^2)(n-1)}{n-k-1}
\]
où $n$ est le nombre d'observations et $k$ est le nombre de variables explicatives. Contrairement à $R^2$, $\overline{R}^2$ pénalise l'ajout de variables explicatives supplémentaires et ne s'améliore que si une nouvelle variable augmente suffisamment la qualité de l'ajustement.

\paragraph{Erreur Quadratique Moyenne (RMSE)} 
Mesure la magnitude moyenne des erreurs de prédiction :
\[
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2} = \sqrt{\frac{\text{SS}_{\text{résiduel}}}{n}}
\]
Le RMSE est exprimé dans les mêmes unités que la variable dépendante, ce qui facilite son interprétation.

\paragraph{Erreur Absolue Moyenne (MAE)}
Représente l'erreur absolue moyenne de prédiction :
\[
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |Y_i - \hat{Y}_i|
\]

\newpage

\subsubsection{Graphique : Valeurs Réelles vs Valeurs Prédites}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{regression_output/06_actual_vs_predicted.png}
    \caption{Performance Prédictive : Valeurs Réelles vs Valeurs Prédites. Ce graphique est crucial pour évaluer la capacité prédictive globale du modèle. Les valeurs réelles observées (axe horizontal) sont tracées en fonction des valeurs prédites par le modèle (axe vertical). Idéalement, tous les points devraient se situer sur la diagonale $Y = \hat{Y}$, indiquant que les prédictions correspondent parfaitement aux observations. En pratique, une dispersion modérée autour de cette ligne est attendue et acceptable. Un motif systématique (par exemple, une courbe ou une divergence croissante) indique que le modèle sous-prédit ou sur-prédit de manière systématique dans certaines régions de l'espace prédictif.}
    \label{fig:actual_vs_pred}
\end{figure}

\textbf{Interprétation des motifs :}
\begin{itemize}
    \item \textbf{Dispersion aléatoire autour de la diagonale} : Bon ajustement du modèle
    \item \textbf{Biais systématique} : Sous-prédiction (points en dessous) ou sur-prédiction (points en dessus) dans certaines régions suggère une mauvaise spécification
    \item \textbf{Éventail s'élargissant} : Indique une hétéroscédasticité avec une variance plus grande pour les valeurs prédites élevées
    \item \textbf{Clusters distincts} : Suggère l'existence de groupes distincts dans les données qui ne sont pas capturés par le modèle
\end{itemize}


\section{Analyse des Points Influents et des Outliers}

% \subsection{Fondements Théoriques : Détection des Observations Aberrantes}

% Une observation aberrante est une observation dont la valeur dévie significativement du motif général des données. Un point influent est une observation qui, si elle est omise de l'analyse, produirait une modification substantielle des résultats de régression. Il est important de noter que tout point aberrant n'est pas forcément influent, et tout point influent n'est pas forcément aberrant.

% \subsubsection{Distance de Cook}

% La distance de Cook est une statistique qui mesure l'influence globale d'une observation particulière sur tous les coefficients estimés du modèle de régression. Elle quantifie la distance euclidienne entre le vecteur de coefficients estimés avec et sans l'observation $i$ :

% \[
% D_i = \frac{(\hat{\beta} - \hat{\beta}_{(-i)})'(X'X)(\hat{\beta} - \hat{\beta}_{(-i)})}{(k+1) \hat{\sigma}^2}
% \]

% où $\hat{\beta}_{(-i)}$ sont les coefficients estimés avec l'observation $i$ omise, et $k+1$ est le nombre de coefficients (incluant l'intercept).

% Une formulation équivalente et plus calculable est :
% \[
% D_i = \frac{r_i^2}{k+1} \cdot \frac{h_{ii}}{1-h_{ii}}
% \]

% où :
% \begin{itemize}
%     \item $r_i$ est le résidu studentisé pour l'observation $i$ (résidu standardisé)
%     \item $h_{ii}$ est le $i$-ème élément diagonal de la matrice de projection $H = X(X'X)^{-1}X'$
% \end{itemize}

% Le terme $\frac{h_{ii}}{1-h_{ii}}$ mesure le "leverage" (la puissance) de l'observation, c'est-à-dire son éloignement dans l'espace des variables explicatives.

% \paragraph{Interprétation de la Distance de Cook :}
% \begin{itemize}
%     \item $D_i < F(0.50; k+1, n-k-1)$ : Observation non influente (règle générale)
%     \item $F(0.25; k+1, n-k-1) < D_i < F(0.50; k+1, n-k-1)$ : Observation potentiellement influente, à examiner
%     \item $D_i > F(0.50; k+1, n-k-1)$ : Observation fortement influente, investigation recommandée
% \end{itemize}

% En pratique, une règle simplifiée souvent utilisée est $D_i > 4/(n-k-1)$, où $n$ est la taille de l'échantillon.

% \subsubsection{Leverage et Résidus Studentisés}

% Le "leverage" d'une observation mesure sa distance extrême dans l'espace des variables explicatives. Une observation avec un leverage élevé a le potentiel d'être influente, mais ne l'est que si elle est également aberrante (résidu important).

% \[
% h_{ii} = x_i (X'X)^{-1} x_i'
% \]

% où $x_i = (1, X_{i1}, X_{i2}, \ldots, X_{ik})$ est le vecteur de variables pour l'observation $i$.

% Le leverage varie entre $1/n$ et 1, avec une moyenne de $(k+1)/n$.

% Les résidus studentisés (also called internally studentized residuals) sont définis par :
% \[
% t_i = \frac{\hat{\epsilon}_i}{\hat{\sigma}\sqrt{1-h_{ii}}}
% \]

% Ces résidus sont standardisés pour tenir compte de la variance hétérogène des résidus bruts.

\newpage

\subsection{Graphique : Distance de Cook}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{regression_output/07_cooks_distance.png}
    \caption{Distance de Cook pour la Détection des Observations Influentes. Ce graphique affiche la distance de Cook pour chaque observation (indexée sur l'axe horizontal). La hauteur de chaque barre verticale représente l'ampleur de l'influence de cette observation particulière sur tous les coefficients estimés du modèle. Une ligne horizontale de référence, généralement positionnée à $4/(n-k-1)$ ou au 50e percentile d'une distribution F, aide à identifier les seuils d'influence problématique. Les observations avec des barres dépassant significativement cette ligne sont des candidats pour une investigation plus approfondie. Ces observations aberrantes peuvent être le résultat d'erreurs de saisie des données, de conditions expérimentales anormales, ou de phénomènes réels importants qui méritent une étude spéciale.}
    \label{fig:cooks}
\end{figure}

\textbf{Démarche d'investigation pour les points influents :}
\begin{enumerate}
    \item \textbf{Vérification des données} : Confirmer que les valeurs aberrantes ne sont pas des erreurs de saisie ou de mesure
    \item \textbf{Analyse contextuelle} : Comprendre les raisons de l'aberrance dans le contexte du domaine d'application
    \item \textbf{Estimation sans outliers} : Réestimer le modèle sans les observations influentes pour évaluer la stabilité des résultats
    % \item
\newpage
\section{Bibliographie}

[1] James, G., Witten, D., Hastie, T., \& Tibshirani, R. (2013). \textit{An Introduction to Statistical Learning: with Applications in R}. Springer.

[2] Harlfoxem. \textit{House Sales in King County, USA}. Kaggle Dataset. Disponible sur : \url{https://www.kaggle.com/datasets/harlfoxem/housesalesprediction}

[3] Wooldridge, J. M. (2016). \textit{Introductory Econometrics: A Modern Approach}. Cengage Learning.

[4] Fox, J. (2016). \textit{Applied Regression Analysis and Generalized Linear Models}. Sage Publications.

[5] Hastie, T., Tibshirani, R., \& Friedman, J. (2009). \textit{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}. Springer.

\end{document}
